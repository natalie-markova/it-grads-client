import { useState, useEffect, useRef, useCallback } from 'react';
import { useNavigate } from 'react-router-dom';
import { useTranslation } from 'react-i18next';
import { Mic, MicOff, Volume2, VolumeX, Award, TrendingUp, TrendingDown, ChevronDown } from 'lucide-react';
import { $api } from '../../../utils/axios.instance';
import toast from 'react-hot-toast';
import Card from '../../ui/Card';
import Section from '../../ui/Section';

interface InterviewSession {
  id: number;
  interviewerPersona: string;
  position: string;
  status: string;
  currentQuestionIndex: number;
}

interface Message {
  id: number;
  role: 'assistant' | 'user';
  content: string;
}

interface FeedbackResult {
  overallScore: number;
  strengths: string[];
  weaknesses: string[];
  feedback: string;
  duration: number;
}

const POSITION_KEYS = [
  'frontend',
  'backend',
  'fullstack',
  'react',
  'nodejs',
  'python',
  'java',
  'devops',
  'qa',
  'dataScientist',
  'dataAnalyst',
  'iosDev',
  'androidDev',
  'uiux',
  'productManager',
  'projectManager',
  'sysAdmin',
  'businessAnalyst',
  'mlEngineer',
  'techWriter'
];

const AudioInterview = () => {
  const { t, i18n } = useTranslation();
  const navigate = useNavigate();
  const [step, setStep] = useState<'setup' | 'interview' | 'feedback'>('setup');
  const [session, setSession] = useState<InterviewSession | null>(null);
  const [messages, setMessages] = useState<Message[]>([]);
  const [currentMessage, setCurrentMessage] = useState('');

  // Audio controls
  const [isListening, setIsListening] = useState(false);
  const [isSpeaking, setIsSpeaking] = useState(false);
  const [audioEnabled, setAudioEnabled] = useState(true);

  // Setup form
  const [persona, setPersona] = useState('friendly_tech');
  const [position, setPosition] = useState('');

  // Feedback
  const [feedback, setFeedback] = useState<FeedbackResult | null>(null);

  // Sending state to prevent double-clicks
  const [isSending, setIsSending] = useState(false);

  // Web Speech API for recognition
  const recognitionRef = useRef<any>(null);
  const audioRef = useRef<HTMLAudioElement | null>(null);
  const isListeningRef = useRef(false); // Ref –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è
  const confirmedTextRef = useRef<string>(''); // –•—Ä–∞–Ω–∏–º –ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
  const restartTimeoutRef = useRef<NodeJS.Timeout | null>(null); // –î–ª—è –æ—Ç–º–µ–Ω—ã —Ç–∞–π–º–µ—Ä–∞ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞
  const isRestartingRef = useRef(false); // –§–ª–∞–≥ —á—Ç–æ –∏–¥—ë—Ç –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫

  // –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Audio –æ–¥–∏–Ω —Ä–∞–∑
  useEffect(() => {
    audioRef.current = new Audio();
    audioRef.current.onended = () => setIsSpeaking(false);
    audioRef.current.onerror = () => setIsSpeaking(false);

    return () => {
      if (audioRef.current) {
        audioRef.current.pause();
        audioRef.current = null;
      }
    };
  }, []);

  // –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Speech Recognition –æ–¥–∏–Ω —Ä–∞–∑ (–±–µ–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –æ—Ç —è–∑—ã–∫–∞)
  useEffect(() => {
    const SpeechRecognition = (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition;
    if (!SpeechRecognition) return;

    const recognition = new SpeechRecognition();
    recognition.continuous = true;
    recognition.interimResults = true;
    recognition.maxAlternatives = 1;

    recognition.onresult = (event: any) => {
      let interimTranscript = '';
      let newFinalTranscript = '';

      for (let i = event.resultIndex; i < event.results.length; i++) {
        const transcript = event.results[i][0].transcript;
        if (event.results[i].isFinal) {
          newFinalTranscript += transcript;
        } else {
          interimTranscript += transcript;
        }
      }

      if (newFinalTranscript) {
        confirmedTextRef.current = (confirmedTextRef.current + ' ' + newFinalTranscript).trim();
        setCurrentMessage(confirmedTextRef.current);
      } else if (interimTranscript) {
        setCurrentMessage((confirmedTextRef.current + ' ' + interimTranscript).trim());
      }
    };

    recognition.onerror = (event: any) => {
      console.warn('Speech recognition error:', event.error);

      // –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏ - –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º
      if (event.error === 'not-allowed' || event.error === 'service-not-allowed') {
        isListeningRef.current = false;
        setIsListening(false);
        toast.error(t('toasts.micAccessDenied'));
        return;
      }

      if (event.error === 'network') {
        isListeningRef.current = false;
        setIsListening(false);
        toast.error(t('toasts.networkError'));
        return;
      }

      // –û—Å—Ç–∞–ª—å–Ω—ã–µ –æ—à–∏–±–∫–∏ (aborted, no-speech, audio-capture) - –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º
      // onend —Å–∞–º –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    };

    recognition.onend = () => {
      // –ï—Å–ª–∏ —ç—Ç–æ –±—ã–ª –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫ - –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º
      if (isRestartingRef.current) {
        isRestartingRef.current = false;
        return;
      }

      // –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –≤—Å—ë –µ—â—ë —Ö–æ—á–µ—Ç —Å–ª—É—à–∞—Ç—å - –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ–º
      if (isListeningRef.current) {
        // –û—Ç–º–µ–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π —Ç–∞–π–º–µ—Ä –µ—Å–ª–∏ –µ—Å—Ç—å
        if (restartTimeoutRef.current) {
          clearTimeout(restartTimeoutRef.current);
        }

        restartTimeoutRef.current = setTimeout(() => {
          if (isListeningRef.current && recognitionRef.current) {
            try {
              isRestartingRef.current = true;
              recognitionRef.current.start();
            } catch (e: any) {
              // –ï—Å–ª–∏ —É–∂–µ –∑–∞–ø—É—â–µ–Ω - —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ
              if (e.message?.includes('already started')) {
                isRestartingRef.current = false;
                return;
              }
              console.error('Failed to restart recognition:', e);
              isListeningRef.current = false;
              setIsListening(false);
            }
          }
        }, 150);
      }
    };

    recognitionRef.current = recognition;

    return () => {
      if (restartTimeoutRef.current) {
        clearTimeout(restartTimeoutRef.current);
      }
      if (recognitionRef.current) {
        try {
          recognitionRef.current.stop();
        } catch (e) {
          // Ignore
        }
      }
    };
  }, []); // –ü—É—Å—Ç–æ–π –º–∞—Å—Å–∏–≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π!

  // –û–±–Ω–æ–≤–ª—è–µ–º —è–∑—ã–∫ recognition –ø—Ä–∏ —Å–º–µ–Ω–µ —è–∑—ã–∫–∞ (–±–µ–∑ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏—è)
  useEffect(() => {
    if (recognitionRef.current) {
      recognitionRef.current.lang = i18n.language === 'en' ? 'en-US' : 'ru-RU';
    }
  }, [i18n.language]);

  // –ú–∞–ø–ø–∏–Ω–≥ –ø–µ—Ä—Å–æ–Ω—ã –Ω–∞ –≥–µ–Ω–¥–µ—Ä –≥–æ–ª–æ—Å–∞
  const personaGender: Record<string, 'male' | 'female'> = {
    strict_hr: 'female',
    friendly_tech: 'male',
    direct_ceo: 'male'
  };

  // –°–∏–Ω—Ç–µ–∑ —Ä–µ—á–∏ —á–µ—Ä–µ–∑ Yandex SpeechKit (–±–µ–∑ –±—Ä–∞—É–∑–µ—Ä–Ω–æ–≥–æ fallback)
  // –ò—Å–ø–æ–ª—å–∑—É–µ–º useRef –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è voiceId, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä–æ–±–ª–µ–º —Å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏ useCallback
  const voiceIdRef = useRef<string | null>(null);

  const speak = useCallback(async (text: string) => {
    if (!audioEnabled) return;
    setIsSpeaking(true);

    try {
      // –ü–µ—Ä–µ–¥–∞—ë–º –≥–µ–Ω–¥–µ—Ä –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤—ã–±—Ä–∞–Ω–Ω–æ–π –ø–µ—Ä—Å–æ–Ω—ã
      const gender = personaGender[persona] || 'male';
      // –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫ –¥–ª—è TTS
      const lang = i18n.language === 'en' ? 'en' : 'ru';

      // –ï—Å–ª–∏ —É –Ω–∞—Å —É–∂–µ –µ—Å—Ç—å –≤—ã–±—Ä–∞–Ω–Ω—ã–π –≥–æ–ª–æ—Å –¥–ª—è —ç—Ç–æ–π —Å–µ—Å—Å–∏–∏ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
      // –ò–Ω–∞—á–µ –ø–µ—Ä–µ–¥–∞—ë–º —Ç–æ–ª—å–∫–æ gender –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –≤—ã–∑–æ–≤–∞
      const requestData: { text: string; gender?: string; voiceId?: string; lang?: string } = { text, lang };

      if (voiceIdRef.current) {
        // –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–π –≥–æ–ª–æ—Å –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –≤ —Ä–∞–º–∫–∞—Ö —Å–µ—Å—Å–∏–∏
        requestData.voiceId = voiceIdRef.current;
      } else {
        // –ü–µ—Ä–≤—ã–π –≤—ã–∑–æ–≤ - –ø–µ—Ä–µ–¥–∞—ë–º gender, —Å–µ—Ä–≤–µ—Ä –≤—ã–±–µ—Ä–µ—Ç —Å–ª—É—á–∞–π–Ω—ã–π –≥–æ–ª–æ—Å
        requestData.gender = gender;
      }

      const response = await $api.post('/interviews/tts', requestData);

      if (response.data.audio && audioRef.current) {
        // –°–æ—Ö—Ä–∞–Ω—è–µ–º voiceId –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ —Å–ª–µ–¥—É—é—â–∏—Ö –∑–∞–ø—Ä–æ—Å–∞—Ö
        if (response.data.voice && !voiceIdRef.current) {
          voiceIdRef.current = response.data.voice.id;
        }

        // –î–µ–∫–æ–¥–∏—Ä—É–µ–º base64 –∞—É–¥–∏–æ –∏ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º
        const audioData = `data:${response.data.format};base64,${response.data.audio}`;
        audioRef.current.src = audioData;
        audioRef.current.play().catch((err) => {
          console.error('Audio playback error:', err);
          setIsSpeaking(false);
          toast.error(t('audioInterview.audioPlaybackError'));
        });
      } else {
        setIsSpeaking(false);
        toast.error(t('audioInterview.ttsUnavailable'));
      }
    } catch (error) {
      console.error('Yandex TTS error:', error);
      setIsSpeaking(false);
      toast.error(t('audioInterview.ttsError'));
    }
  }, [audioEnabled, t, persona, i18n.language]);

  const startListening = () => {
    if (!recognitionRef.current) {
      toast.error(t('audioInterview.speechNotSupported'));
      return;
    }

    // –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç
    confirmedTextRef.current = '';
    setCurrentMessage('');

    // –û—Ç–º–µ–Ω—è–µ–º –ª—é–±–æ–π pending —Ç–∞–π–º–µ—Ä –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞
    if (restartTimeoutRef.current) {
      clearTimeout(restartTimeoutRef.current);
      restartTimeoutRef.current = null;
    }

    // –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ñ–ª–∞–≥–∏ –ü–ï–†–ï–î –∑–∞–ø—É—Å–∫–æ–º
    isListeningRef.current = true;
    isRestartingRef.current = false;
    setIsListening(true);

    try {
      recognitionRef.current.start();
    } catch (e: any) {
      // –ï—Å–ª–∏ —É–∂–µ –∑–∞–ø—É—â–µ–Ω - —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ
      if (e.message?.includes('already started')) {
        return;
      }
      console.error('Failed to start recognition:', e);
      isListeningRef.current = false;
      setIsListening(false);
    }
  };

  const stopListening = () => {
    // –°–Ω–∞—á–∞–ª–∞ —Å–±—Ä–∞—Å—ã–≤–∞–µ–º –≤—Å–µ —Ñ–ª–∞–≥–∏
    isListeningRef.current = false;
    isRestartingRef.current = false;

    // –û—Ç–º–µ–Ω—è–µ–º —Ç–∞–π–º–µ—Ä –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞
    if (restartTimeoutRef.current) {
      clearTimeout(restartTimeoutRef.current);
      restartTimeoutRef.current = null;
    }

    setIsListening(false);

    if (recognitionRef.current) {
      try {
        recognitionRef.current.stop();
      } catch (e) {
        // Ignore - –º–æ–∂–µ—Ç –±—ã—Ç—å —É–∂–µ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
      }
    }
  };

  const startInterview = async () => {
    if (!position) {
      toast.error(t('audioInterview.selectPositionError'));
      return;
    }

    try {
      // –°–±—Ä–∞—Å—ã–≤–∞–µ–º –≥–æ–ª–æ—Å –¥–ª—è –Ω–æ–≤–æ–π —Å–µ—Å—Å–∏–∏
      voiceIdRef.current = null;

      const response = await $api.post('/interviews/audio', {
        interviewerPersona: persona,
        position: t(`audioInterview.positions.${position}`),
        lang: i18n.language === 'en' ? 'en' : 'ru'
      });

      setSession(response.data.session);
      setMessages([response.data.firstMessage]);
      setStep('interview');

      speak(response.data.firstMessage.content);

      toast.success(t('audioInterview.interviewStarted'));
    } catch (error) {
      console.error('Error starting interview:', error);
      toast.error(t('audioInterview.interviewCreateError'));
    }
  };

  const sendAnswer = async () => {
    if (!currentMessage.trim() || !session || isSending) return;

    // –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç–≤–µ—Ç –∏ —Å—Ä–∞–∑—É –æ—á–∏—â–∞–µ–º –ø–æ–ª–µ –≤–≤–æ–¥–∞
    const answerToSend = currentMessage.trim();
    setCurrentMessage('');
    setIsSending(true);

    // –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–ø–∏—Å—å –µ—Å–ª–∏ –æ–Ω–∞ –∞–∫—Ç–∏–≤–Ω–∞
    if (isListening) {
      stopListening();
    }

    try {
      const response = await $api.post(`/interviews/audio/${session.id}/answer`, {
        content: answerToSend
      });

      setMessages(prev => [...prev, response.data.userMessage, response.data.aiMessage]);

      // Speak next question
      speak(response.data.aiMessage.content);

      // Update session
      setSession({
        ...session,
        currentQuestionIndex: response.data.questionNumber
      });

      // Check if interview is complete
      if (response.data.isLastQuestion) {
        setTimeout(() => {
          completeInterview();
        }, 3000);
      }
    } catch (error) {
      console.error('Error sending answer:', error);
      toast.error(t('audioInterview.answerSendError'));
      // –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –ø—Ä–∏ –æ—à–∏–±–∫–µ
      setCurrentMessage(answerToSend);
    } finally {
      setIsSending(false);
    }
  };

  const completeInterview = async () => {
    if (!session) return;

    try {
      const response = await $api.post(`/interviews/audio/${session.id}/complete`);
      setFeedback(response.data);
      setStep('feedback');

      speak(response.data.feedback);
    } catch (error) {
      console.error('Error completing interview:', error);
      toast.error(t('audioInterview.interviewCompleteError'));
    }
  };

  const toggleAudio = () => {
    setAudioEnabled(!audioEnabled);
    if (audioEnabled) {
      if (audioRef.current) {
        audioRef.current.pause();
      }
      setIsSpeaking(false);
    }
  };

  const personaInfo: Record<string, { icon: string; gender: 'male' | 'female' }> = {
    strict_hr: { icon: 'üëî', gender: 'female' },
    friendly_tech: { icon: 'üë®‚Äçüíª', gender: 'male' },
    direct_ceo: { icon: 'üíº', gender: 'male' }
  };

  return (
    <div className="min-h-screen bg-dark-bg text-white py-8">
      <div className="max-w-4xl mx-auto px-4 sm:px-6 lg:px-8">
        {/* Header */}
        <Section title={`üéôÔ∏è ${t('audioInterview.title')}`} subtitle={t('audioInterview.subtitle')} className="bg-dark-bg py-0 mb-8" />

        {/* Setup Step */}
        {step === 'setup' && (
          <Card>
            <h2 className="text-2xl font-bold text-white mb-6">{t('audioInterview.setupTitle')}</h2>

            {/* Position Select */}
            <div className="mb-6">
              <label className="block text-gray-300 mb-2">{t('audioInterview.desiredPosition')}</label>
              <div className="relative">
                <select
                  value={position}
                  onChange={(e) => setPosition(e.target.value)}
                  className="w-full px-4 py-3 bg-dark-surface border border-gray-700 rounded-lg text-white focus:ring-2 focus:ring-accent-cyan focus:border-transparent outline-none appearance-none cursor-pointer"
                >
                  <option value="">{t('audioInterview.selectPosition')}</option>
                  {POSITION_KEYS.map((key) => (
                    <option key={key} value={key}>{t(`audioInterview.positions.${key}`)}</option>
                  ))}
                </select>
                <ChevronDown className="absolute right-4 top-1/2 -translate-y-1/2 w-5 h-5 text-gray-400 pointer-events-none" />
              </div>
            </div>

            {/* Persona Selection */}
            <div className="mb-8">
              <label className="block text-gray-300 mb-3">{t('audioInterview.chooseInterviewer')}</label>
              <div className="grid grid-cols-1 md:grid-cols-3 gap-4">
                {Object.entries(personaInfo).map(([key, info]) => (
                  <button
                    key={key}
                    onClick={() => setPersona(key)}
                    className={`p-4 rounded-lg border-2 transition-all ${
                      persona === key
                        ? 'border-accent-cyan bg-accent-cyan/20 text-white'
                        : 'border-gray-700 bg-dark-surface text-gray-300 hover:border-accent-cyan/50 hover:bg-dark-card'
                    }`}
                  >
                    <div className="text-4xl mb-2">{info.icon}</div>
                    <div className="font-semibold mb-1">{t(`audioInterview.personas.${key}.name`)}</div>
                    <div className="text-xs text-gray-400">{t(`audioInterview.personas.${key}.description`)}</div>
                  </button>
                ))}
              </div>
            </div>

            {/* Start Button */}
            <button
              onClick={startInterview}
              className="w-full btn-primary"
            >
              {t('audioInterview.startInterview')}
            </button>
          </Card>
        )}

        {/* Interview Step */}
        {step === 'interview' && (
          <Card>
            {/* Progress */}
            <div className="mb-6">
              <div className="flex justify-between items-center mb-2">
                <span className="text-sm text-gray-400">{t('audioInterview.progress')}</span>
                <span className="text-sm text-gray-400">
                  {t('audioInterview.questionOf', { current: session?.currentQuestionIndex || 0, total: 5 })}
                </span>
              </div>
              <div className="w-full bg-dark-surface rounded-full h-2">
                <div
                  className="bg-accent-cyan h-2 rounded-full transition-all"
                  style={{ width: `${((session?.currentQuestionIndex || 0) / 5) * 100}%` }}
                />
              </div>
            </div>

            {/* Messages */}
            <div className="mb-6 max-h-96 overflow-y-auto space-y-4">
              {messages.map((msg, idx) => (
                <div
                  key={idx}
                  className={`p-4 rounded-lg ${
                    msg.role === 'assistant'
                      ? 'bg-accent-cyan/20 border border-accent-cyan/50'
                      : 'bg-dark-surface ml-12 border border-gray-700'
                  }`}
                >
                  <div className="text-xs text-gray-400 mb-1">
                    {msg.role === 'assistant' ? t('audioInterview.interviewer') : t('audioInterview.you')}
                  </div>
                  <div className="text-gray-300">{msg.content}</div>
                </div>
              ))}
            </div>

            {/* Audio Controls */}
            <div className="flex gap-4 mb-4">
              <button
                onClick={toggleAudio}
                className="btn-secondary px-4 py-2"
                title={audioEnabled ? t('audioInterview.disableSound') : t('audioInterview.enableSound')}
              >
                {audioEnabled ? <Volume2 size={20} /> : <VolumeX size={20} />}
              </button>

              <div className="flex-1 relative flex items-center">
                <input
                  type="text"
                  value={currentMessage}
                  onChange={(e) => setCurrentMessage(e.target.value)}
                  placeholder={t('audioInterview.writeAnswer')}
                  className="input-field pr-14 w-full"
                  onKeyPress={(e) => e.key === 'Enter' && sendAnswer()}
                />
                <button
                  onClick={isListening ? stopListening : startListening}
                  className={`absolute right-2 top-1/2 -translate-y-1/2 p-2 rounded-lg transition-all ${
                    isListening
                      ? 'bg-red-500 animate-pulse text-white'
                      : 'bg-accent-cyan hover:bg-accent-cyan/80 text-dark-bg'
                  }`}
                  title={isListening ? t('audioInterview.stopRecording') : t('audioInterview.startRecording')}
                >
                  {isListening ? <MicOff size={20} /> : <Mic size={20} />}
                </button>
              </div>

              <button
                onClick={sendAnswer}
                disabled={!currentMessage.trim() || isSpeaking || isSending}
                className="btn-primary disabled:opacity-50 disabled:cursor-not-allowed"
              >
                {isSending ? '...' : t('audioInterview.send')}
              </button>
            </div>

            {isSpeaking && (
              <div className="text-center text-sm text-accent-cyan animate-pulse mb-2">
                üîä {t('audioInterview.interviewerSpeaking')}
              </div>
            )}

            {isListening && (
              <div className="text-center text-sm text-red-400 animate-pulse mb-2">
                üéôÔ∏è {t('audioInterview.listening')}
              </div>
            )}

            {/* –ö–Ω–æ–ø–∫–∞ –¥–æ—Å—Ä–æ—á–Ω–æ–≥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è */}
            <div className="mt-6 text-center">
              <button
                onClick={completeInterview}
                className="px-6 py-2 bg-red-500/20 border border-red-500/50 text-red-400 rounded-lg hover:bg-red-500/30 transition-all text-sm"
              >
                {t('audioInterview.endEarly')}
              </button>
            </div>
          </Card>
        )}

        {/* Feedback Step */}
        {step === 'feedback' && feedback && (
          <Card>
            <div className="text-center mb-8">
              <Award className="w-16 h-16 mx-auto mb-4 text-accent-gold" />
              <h2 className="text-3xl font-bold text-white mb-2">{t('audioInterview.interviewComplete')}</h2>
              <div className="text-5xl font-bold text-accent-cyan mb-2">
                {feedback.overallScore}/100
              </div>
              <div className="text-gray-400">
                {t('audioInterview.duration')}: {Math.floor(feedback.duration / 60)} {t('audioInterview.min')} {feedback.duration % 60} {t('audioInterview.sec')}
              </div>
            </div>

            {/* Strengths */}
            <div className="mb-6">
              <div className="flex items-center gap-2 mb-3">
                <TrendingUp className="text-green-400" />
                <h3 className="text-xl font-semibold text-white">{t('audioInterview.strengths')}</h3>
              </div>
              <ul className="space-y-2">
                {feedback.strengths.map((strength, idx) => (
                  <li key={idx} className="flex items-start gap-2">
                    <span className="text-green-400 mt-1">‚úì</span>
                    <span className="text-gray-300">{strength}</span>
                  </li>
                ))}
              </ul>
            </div>

            {/* Weaknesses */}
            <div className="mb-6">
              <div className="flex items-center gap-2 mb-3">
                <TrendingDown className="text-orange-400" />
                <h3 className="text-xl font-semibold text-white">{t('audioInterview.improvements')}</h3>
              </div>
              <ul className="space-y-2">
                {feedback.weaknesses.map((weakness, idx) => (
                  <li key={idx} className="flex items-start gap-2">
                    <span className="text-orange-400 mt-1">!</span>
                    <span className="text-gray-300">{weakness}</span>
                  </li>
                ))}
              </ul>
            </div>

            {/* Detailed Feedback */}
            <div className="mb-8 p-4 bg-dark-surface rounded-lg border border-gray-700">
              <h3 className="text-xl font-semibold text-white mb-2">{t('audioInterview.interviewerComment')}</h3>
              <p className="text-gray-300">{feedback.feedback}</p>
            </div>

            {/* Actions */}
            <div className="flex gap-4">
              <button
                onClick={() => {
                  setStep('setup');
                  setSession(null);
                  setMessages([]);
                  setCurrentMessage('');
                  setFeedback(null);
                  voiceIdRef.current = null;
                }}
                className="btn-primary flex-1"
              >
                {t('audioInterview.newInterview')}
              </button>
              <button
                onClick={() => navigate('/home')}
                className="btn-secondary flex-1"
              >
                {t('audioInterview.toHome')}
              </button>
            </div>
          </Card>
        )}
      </div>
    </div>
  );
};

export default AudioInterview;